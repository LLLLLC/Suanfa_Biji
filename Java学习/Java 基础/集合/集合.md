# 1. HashMap

## 1.1 HashMap 底层的变化

### JDK1.8 之前

JDK1.8 之前 HashMap 底层是 **数组和链表** 结合在一起使用也就是 **链表散列**。

HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 `(n - 1) & hash` 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。

所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。

![jdk1.8之前的内部结构.png](../../images/jdk1.8之前的内部结构.png)



### JDK1.8 之后

相比于之前的版本，JDK1.8 以后在解决哈希冲突时有了较大的变化。

当链表长度大于阈值（默认为 8）时，会首先调用 `treeifyBin()`方法。这个方法会根据 HashMap 数组来决定是否转换为红黑树。只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是执行 `resize()` 方法对数组扩容。

![up-bba283228693dae74e78da1ef7a9a04c684.webp](../../images/up-bba283228693dae74e78da1ef7a9a04c684.webp)

## 1.2 HashMap 的主要组成

- Node

  ```java
  // 继承自 Map.Entry<K,V>
  static class Node<K,V> implements Map.Entry<K,V> {
         final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较
         final K key;//键
         V value;//值
         // 指向下一个节点
         Node<K,V> next;
         Node(int hash, K key, V value, Node<K,V> next) {
              this.hash = hash;
              this.key = key;
              this.value = value;
              this.next = next;
          }
          public final K getKey()        { return key; }
          public final V getValue()      { return value; }
          public final String toString() { return key + "=" + value; }
          // 重写hashCode()方法
          public final int hashCode() {
              return Objects.hashCode(key) ^ Objects.hashCode(value);
          }
  
          public final V setValue(V newValue) {
              V oldValue = value;
              value = newValue;
              return oldValue;
          }
          // 重写 equals() 方法
          public final boolean equals(Object o) {
              if (o == this)
                  return true;
              if (o instanceof Map.Entry) {
                  Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                  if (Objects.equals(key, e.getKey()) &&
                      Objects.equals(value, e.getValue()))
                      return true;
              }
              return false;
          }
  }
  ```

  TreeNode:

  ```java
  // 继承自 Map.Entry<K,V>
  static class Node<K,V> implements Map.Entry<K,V> {
         final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较
         final K key;//键
         V value;//值
         // 指向下一个节点
         Node<K,V> next;
         Node(int hash, K key, V value, Node<K,V> next) {
              this.hash = hash;
              this.key = key;
              this.value = value;
              this.next = next;
          }
          public final K getKey()        { return key; }
          public final V getValue()      { return value; }
          public final String toString() { return key + "=" + value; }
          // 重写hashCode()方法
          public final int hashCode() {
              return Objects.hashCode(key) ^ Objects.hashCode(value);
          }
  
          public final V setValue(V newValue) {
              V oldValue = value;
              value = newValue;
              return oldValue;
          }
          // 重写 equals() 方法
          public final boolean equals(Object o) {
              if (o == this)
                  return true;
              if (o instanceof Map.Entry) {
                  Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                  if (Objects.equals(key, e.getKey()) &&
                      Objects.equals(value, e.getValue()))
                      return true;
              }
              return false;
          }
  }
  ```

  

- **loadFactor 加载因子**

  loadFactor 加载因子是控制数组存放数据的疏密程度，loadFactor 越趋近于 1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor 越小，也就是趋近于 0，数组中存放的数据(entry)也就越少，也就越稀疏。

  **loadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值**。

  给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。

- **threshold**

  **threshold = capacity \* loadFactor**，**当 Size>=threshold**的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 **衡量数组是否需要扩增的一个标准**。

## 1.3 HashMap 扩容

主要留意以下几点：

- `HashMap` 默认的初始化大小为 16。默认负载因子是0.75。
- 之后每次扩充，容量变为原来的 2 倍。
- `HashMap` 总是使用 2 的幂作为哈希表的大小。
- 当链表长度大于阈值（默认为 8）时，会首先调用 `treeifyBin()`方法。这个方法会根据 HashMap 数组来决定是否转换为红黑树。只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是执行 `resize()` 方法对数组扩容。
- HashMap 的长度一定是2的幂次方，原因是  hashCode % n = hashCode & (n - 1)。
- HashMap 如何保证数组大小是2的幂次方的呢？其中初始化给定大小的时候有个 `tableSizeFor` 方法，该方法指定数组大小肯定是2的幂次方。比如 5 ---> 8，9--->16，最大为 1 << 30。默认为16，然后就下次扩容的时候为2倍，则一定是2的幂次方。



## 1.8 HashMap 的长度为什么是 2 的幂次方

为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ `(n - 1) & hash`”。（n 代表数组长度）。这也就解释了 HashMap 的长度为什么是 2 的幂次方。

**这个算法应该如何设计呢？**

- 我们首先可能会想到采用%取余的操作来实现。但是，重点来了：**“取余(%)操作中如果除数是 2 的幂次则等价于与其除数减一的与(&)操作（也就是说 hash%length==hash&(length-1)的前提是 length 是 2 的 n 次方；）。”** 并且 **采用二进制位操作 &，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是 2 的幂次方。**

- **HashMap 扩容的时候，会使用的是2次幂的扩展(指长度扩为原来2倍)。所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。**

  因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图：

![d7acbad8-d941-11e4-9493-2c5e69d084c0.png](../../images/d7acbad8-d941-11e4-9493-2c5e69d084c0.png)

# 2. ArrayList

## 2.1 ArrayList 扩容原理

> 说下ArrayList 扩容的原理吧？
>
> 如果开始不指定容量：
>
> 默认为空数组，添加第一个元素的时候，长度会修改为 10 
>
> 如果指定容量，使用指定的容量
>
> 如果是 new ArrayList<>(Collection); 会使用 Collection 的容量
>
> `elementData = collection.toArray();`
>
> 如果 collection 为空，则使用空数组
>
> 
>
> 添加元素的时候，如果 mini_size > size，size + size * 0.5(位运算，实际向下取整)。
>
> 如果 newSize 还是小于 mini_size，那么直接使用 mini_size。
>
> 如果newCapacity 超过了 MAX_ARRAY_SIZE（Integer.MAX_VALUE - 8），会执行 hugeCapacity 方法，
>
> - 如果 mini_size > MAX_ARRAY_SIZE，size = Integer.MAX_VALUE，否则 size = MAX_ARRAY_SIZE
>
> 扩容的 `grow(int minCapacity)`  方法，如果 mini_size > size，需要使用到 
>
> ```java
> elementData = Arrays.copyOf(elementData, newCapacity);
> ```
>
> ————————————————————————————————————————————————
>
> 额外的，还有其它两个用法
>
> System.arraycopy() 和 Arrays.copyOf()
>
> 其中 System.arraycopy() 是复制源数组到目标数组中，Arrays.copyOf() 底层其实还是 System.arraycopy()
>
> ```
>     // 我们发现 arraycopy 是一个 native 方法,接下来我们解释一下各个参数的具体意义
>     /**
>     *   复制数组
>     * @param src 源数组
>     * @param srcPos 源数组中的起始位置
>     * @param dest 目标数组
>     * @param destPos 目标数组中的起始位置
>     * @param length 要复制的数组元素的数量
>     */
>     public static native void arraycopy(Object src,  int  srcPos,
>                                         Object dest, int destPos,
>                                         int length);
> ```
>
> Arrays.copyOf() 只是在底层 new 了一个新的数组，调用了System.arraycopy()方法
>
> ```
>     public static int[] copyOf(int[] original, int newLength) {
>     	// 申请一个新的数组
>         int[] copy = new int[newLength];
> 	// 调用System.arraycopy,将源数组中的数据进行拷贝,并返回新的数组
>         System.arraycopy(original, 0, copy, 0,
>                          Math.min(original.length, newLength));
>         return copy;
>     }
> ```
>
> 
>
> 还有一个 ensureCapacity 方法，可以提前分配好容量
>
> 理论上来说，最好在向 `ArrayList` 添加大量元素之前用 `ensureCapacity` 方法，以减少增量重新分配的次数
>
> 实际上效果有限
>
> ```
> public class EnsureCapacityTest {
>     public static void main(String[] args) {
>         ArrayList<Object> list = new ArrayList<Object>();
>         final int N = 10000000;
>         long startTime1 = System.currentTimeMillis();
>         list.ensureCapacity(N);
>         for (int i = 0; i < N; i++) {
>             list.add(i);
>         }
>         long endTime1 = System.currentTimeMillis();
>         System.out.println("使用ensureCapacity方法后："+(endTime1 - startTime1));
>     }
> }
> 
> ```

# 3.  ConcurrentHashMap

参考：https://javaguide.cn/java/collection/concurrent-hash-map-source-code.html#_2-%E5%88%9D%E5%A7%8B%E5%8C%96



可以分为1.7 和 1.8 两种情况

> 其中 1.7 采用的是分片为segment 的方式，而1.8采用的是锁住 node 的头节点

## 3.1 JDK 1.7

**重点**

- 默认 Segment 的数目为16
- 默认 容量大小也为16
- 一个Segment 可以理解为并发操作单元
- 一旦制定了Segment 的数目，就不能再改变

`ConcurrnetHashMap` 有多个 `segment` 组成，而每一个 `Segment` 是一个类似于 `HashMap` 的结构，所以每一个 `HashMap` 的内部可以进行扩容。但是 `Segment` 的个数一旦**初始化就不能改变**，默认 `Segment` 的个数是 16 个，可以认为 `ConcurrentHashMap` 默认支持最多 16 个线程并发。

**初始化**

```
    public ConcurrentHashMap() {
        this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);
    }
    
        /**
     * 默认初始化容量
     */
    static final int DEFAULT_INITIAL_CAPACITY = 16;

    /**
     * 默认负载因子
     */
    static final float DEFAULT_LOAD_FACTOR = 0.75f;

    /**
     * 默认并发级别
     */
    static final int DEFAULT_CONCURRENCY_LEVEL = 16;

```

**源码初始化逻辑：**

1. 校验并发级别 `concurrencyLevel` 大小，如果大于最大值，重置为最大值。无参构造**默认值是 16.**
2. 寻找并发级别 `concurrencyLevel` 之上最近的 **2 的幂次方**值，作为初始化容量大小，**默认是 16**。
3. 记录 `segmentShift` 偏移量，这个值为【容量 = 2 的 N 次方】中的 N，在后面 Put 时计算位置时会用到。**默认是 32 - sshift = 28**  / **sshift** =4
4. 记录 `segmentMask`，默认是 ssize - 1 = 16 -1 = 15.
5. **初始化 `segments[0]`**，**默认大小为 2**，**负载因子 0.75**，**扩容阀值是 2\*0.75=1.5**，插入第二个值时才会进行扩容。



**Put 方法的源码**

1. 计算要 put 的 key 的位置，获取指定位置的 `Segment`。

2. 如果指定位置的 `Segment` 为空，则初始化这个 `Segment`.

   **初始化 Segment 流程：**

   1. 检查计算得到的位置的 `Segment` 是否为 null.
   2. 为 null 继续初始化，使用 `Segment[0]` 的容量和负载因子创建一个 `HashEntry` 数组。
   3. 再次检查计算得到的指定位置的 `Segment` 是否为 null.
   4. 使用创建的 `HashEntry` 数组初始化这个 Segment.
   5. 自旋判断计算得到的指定位置的 `Segment` 是否为 null，使用 CAS 在这个位置赋值为 `Segment`.

3. `Segment.put` 插入 key,value 值。



**上述第三步 `Segment.put` key, value 的详细逻辑**

1. `tryLock()` 获取锁，获取不到使用 **`scanAndLockForPut`** 方法继续获取。
2. 计算 put 的数据要放入的 index 位置，然后获取这个位置上的 `HashEntry` 。
3. 遍历 put 新元素，为什么要遍历？因为这里获取的 `HashEntry` 可能是一个空元素，也可能是链表已存在，所以要区别对待。
4. 如果这个位置上的 **`HashEntry` 不存在**：
   1. 如果当前容量大于扩容阀值，小于最大容量，**进行扩容**。
   2. 直接头插法插入。
5. 如果这个位置上的 **`HashEntry` 存在**：
   1. 判断链表当前元素 key 和 hash 值是否和要 put 的 key 和 hash 值一致。一致则替换值
   2. 不一致，获取链表下一个节点，直到发现相同进行值替换，或者链表表里完毕没有相同的。 
      1. 如果当前容量大于扩容阀值，小于最大容量，**进行扩容**。
      2. 直接链表头插法插入。
6. 如果要插入的位置之前已经存在，替换后返回旧值，否则返回 null.

这里面的第一步中的 `scanAndLockForPut` 操作这里没有介绍，这个方法做的操作就是不断的自旋 `tryLock()` 获取锁。当自旋次数大于指定次数时，使用 `lock()` 阻塞获取锁。在自旋时顺表获取下 hash 位置的 `HashEntry`。





